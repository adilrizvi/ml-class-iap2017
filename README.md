# A Whirlwind Tour of ML
###_IAP 2017 course at MIT_

## About
This course gives a high-level overview of diverse areas of machine learning. The goal is to introduce students to core concepts and techniques in ML, and provide enough of a primer on different sub-areas of ML so that students can choose the right approach for a given problem and explore interesting topics further.

The course covers an introduction to ML, Inference, Bayesian Methods and Neural Networks. Each class is taught by graduate students or post-docs at MIT working in the specific areas.

Organized by [Manasi Vartak](http://people.csail.mit.edu/mvartak/) and [Maggie Makar](http://mmakar.scripts.mit.edu/mmakar/) from MIT CSAIL.

## Session I: Introduction to ML
This session gives an overview of supervised and unsupervised learning, and an introduction to probabilistic graphical models.

_Concepts_: Loss functions, Linear regression, Logistic regression, SVMs, Decision trees, Random Forests, Clustering, PCA, Graphical Models, Variable Elimination 

Taught by [Manasi Vartak](http://people.csail.mit.edu/mvartak/).

### [Slides](slides/lec1.pdf)
### Resources
- [MIT 6.867 Machine Learning](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/)
- [Coursera Machine Learning](https://www.coursera.org/learn/machine-learning)
- [MIT 9.520 Statistical Learning Theory](http://www.mit.edu/~9.520/fall16/)
- [CMU: Intro to Machine Learning](www.cs.cmu.edu/~epxing/Class/10701/)
- [Michael Jordan Review of Graphical Models](https://www.cs.cmu.edu/~aarti/Class/10701/readings/graphical_model_Jordan.pdf)
- [Coursera Probabilistic Graphical Models](https://www.coursera.org/learn/probabilistic-graphical-models/home)
- [Columbia University: Probabilistic Graphical Models](http://www.cs.columbia.edu/~blei/fogm/2016F/)

## Session II: Inference
This session gives an overview of (approximate) inference for probabilistic graphical models.

_Concepts_: Gaussian Mixture Models, Variational Inference, Monte Carlo Sampling

Taught by [Maggie Makar](http://mmakar.scripts.mit.edu/mmakar/)
###[Slides](slides/lec2.pdf)
### Resources
- [Tutorial on VI](http://digitalassets.lib.berkeley.edu/techreports/ucb/text/CSD-98-980.pdf)
- [A Review of recent work on VI](https://arxiv.org/pdf/1602.05221v2.pdf), Section 5
- [Tutorial on Sampling methods](http://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf)
- [A review (and really cool demos) of recent work on sampling](http://chifeng.scripts.mit.edu/stuff/mcmc-demo/)

## Session III: Bayesian Methods
This session gives a whirlwind tour of Bayesian Methods in ML.

_Concepts_: What does it mean to be Bayesian in ML, Why be Bayesian, Posterior Inference, Parameteric vs. Non-Parametric Bayes

Taught by [Trevor Campbell](http://trevorcampbell.me/)
###[Slides](https://docs.google.com/presentation/d/1qSYB8iDMIEInr0b4pn2M4Q0fSyFzyCBKFmO7j-6oLmM/edit#slide=id.p)
### Resources
See slides!

## Session IV: Neural Networks
This session gives an overview of neural networks, particularly as applied to computer vision.

_Concepts_: Neural Nets, Convolutional NNs, AlexNet, GoogleLeNet, Transfer learning

Taught by [Carl Vondrick](http://web.mit.edu/vondrick/)
###[Slides](http://6.869.csail.mit.edu/fa15/lecture/6.869-DeepLearningApplications3.pdf)
Note: these slides are not exactly the ones that were presented in class. Please feel free to reach out to Carl if you need information that's not in these slides.
